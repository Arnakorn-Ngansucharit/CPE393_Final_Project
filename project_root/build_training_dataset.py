# build_training_dataset.py

from pathlib import Path
import pandas as pd
from datetime import datetime

BASE_DIR = Path(__file__).resolve().parent

HOURLY_DIR = BASE_DIR / "data" / "clean" / "hourly"
PROCESSED_DIR = BASE_DIR / "data" / "processed"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

# ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ fix; ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° timestamp ‡πÉ‡∏ô main()
# OUTPUT_CSV = PROCESSED_DIR / "aqi_lagged_SEA.csv"


def load_all_daily() -> pd.DataFrame:
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå hourly ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å HOURLY_DIR
    ‡πÅ‡∏•‡πâ‡∏ß concat ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏ö row ‡∏ã‡πâ‡∏≥
    """
    files = sorted(HOURLY_DIR.glob("waqi_cleaned_*.csv"))
    if not files:
        raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö hourly files ‡πÉ‡∏ô {HOURLY_DIR}")

    # ‡∏Ç‡πâ‡∏≠ 2: ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå
    print(f"[TRAIN-DATA] ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå hourly ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á training dataset ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
    for f in files:
        try:
            rel = f.relative_to(BASE_DIR)
        except ValueError:
            rel = f
        print(f"   - {rel}")

    dfs = []
    for f in files:
        print(f"[TRAIN-DATA] ‡πÇ‡∏´‡∏•‡∏î {f}")
        df = pd.read_csv(f)
        dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)

    # ‡∏Ç‡πâ‡∏≠ 3: ‡∏•‡∏ö row ‡∏ã‡πâ‡∏≥
    before = len(df_all)
    subset = [c for c in ["station_idx", "station_time"] if c in df_all.columns]
    if subset:
        df_all = df_all.drop_duplicates(subset=subset)
        after = len(df_all)
        print(f"[TRAIN-DATA] ‡∏•‡∏ö row ‡∏ã‡πâ‡∏≥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏µ‡∏¢‡πå {subset}: {before} ‚Üí {after}")
    else:
        df_all = df_all.drop_duplicates()
        after = len(df_all)
        print(f"[TRAIN-DATA] ‡∏•‡∏ö row ‡∏ã‡πâ‡∏≥ (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå): {before} ‚Üí {after}")

    if "station_time" not in df_all.columns:
        raise ValueError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå station_time ‡πÉ‡∏ô hourly data")

    df_all["station_time"] = pd.to_datetime(df_all["station_time"], errors="coerce")
    df_all = df_all.dropna(subset=["station_time"])

    # sort ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô time series ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
    sort_cols = [c for c in ["station_idx", "station_time"] if c in df_all.columns]
    df_all = df_all.sort_values(sort_cols).reset_index(drop=True)

    print(f"[TRAIN-DATA] ‡∏£‡∏ß‡∏° hourly data ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df_all)} ‡πÅ‡∏ñ‡∏ß (‡∏´‡∏•‡∏±‡∏á sort ‡πÅ‡∏•‡πâ‡∏ß)")
    return df_all


def add_lag_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    cols_keep = [
        "station_idx", "station_name", "lat", "lon",
        "station_time", "aqi", "pm25", "pm10", "o3", "no2", "so2", "co",
        "t", "h", "p", "w",
    ]
    cols_keep = [c for c in cols_keep if c in df.columns]
    df = df[cols_keep]

    def create_lags(group: pd.DataFrame) -> pd.DataFrame:
        group = group.sort_values("station_time")

        # target: AQI ‡πÉ‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        group["aqi_next1h"] = group["aqi"].shift(-1)

        # lag features ‡∏´‡∏•‡∏±‡∏Å ‡πÜ
        group["aqi_lag1"] = group["aqi"].shift(1)
        group["aqi_lag3"] = group["aqi"].shift(3)

        if "pm25" in group.columns:
            group["pm25_lag1"] = group["pm25"].shift(1)
        if "pm10" in group.columns:
            group["pm10_lag1"] = group["pm10"].shift(1)
        if "t" in group.columns:
            group["t_lag1"] = group["t"].shift(1)
        if "h" in group.columns:
            group["h_lag1"] = group["h"].shift(1)

        return group

    if "station_idx" in df.columns:
        df = df.groupby("station_idx", group_keys=False).apply(create_lags)
    else:
        df = create_lags(df)

    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà target ‡∏´‡∏£‡∏∑‡∏≠ lag ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô NaN
    must_have = [c for c in ["aqi_next1h", "aqi_lag1"] if c in df.columns]
    df = df.dropna(subset=must_have)

    print(f"[TRAIN-DATA] ‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥ lag + target ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(df)} ‡πÅ‡∏ñ‡∏ß ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô")
    return df


def main():
    df_all = load_all_daily()
    df_lagged = add_lag_features(df_all)

    # ‡∏Ç‡πâ‡∏≠ 1: ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå training dataset ‡∏ï‡∏≤‡∏° timestamp ‡∏ï‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_csv = PROCESSED_DIR / f"aqi_lagged_SEA_{ts}.csv"

    df_lagged.to_csv(output_csv, index=False)
    print(f"üéâ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training dataset ‡∏ó‡∏µ‡πà: {output_csv}")


if __name__ == "__main__":
    main()