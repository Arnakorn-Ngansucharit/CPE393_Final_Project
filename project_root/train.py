import os
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv # Import for loading .env file
import joblib

import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from mlflow.tracking import MlflowClient

# ================= DAGSHUB / MLFLOW CONFIGURATION =================

# 1. Load environment variables from .env file
# This loads MLFLOW_TRACKING_URI, USERNAME, and PASSWORD from the .env file
load_dotenv()

# 2. Check and set MLflow Tracking URI using Environment Variables
if os.environ.get("MLFLOW_TRACKING_URI") is not None:
    # Set the URI before calling set_experiment()
    mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
    print(f"‚úÖ MLflow Tracking URI set to: {mlflow.get_tracking_uri()}")
else:
    print("‚ö†Ô∏è MLFLOW_TRACKING_URI not set in .env or Environment. Logging to local .mlruns.")

# ================= END OF DAGSHUB CONFIGURATION =================


# ================= CONFIG =================

BASE_DIR = Path(__file__).resolve().parent
# Note: You need to ensure the data/processed directory exists and contains data
DATA_DIR = BASE_DIR / "data" / "processed" 
MODELS_DIR = BASE_DIR / "models"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Define TRAINED_DATA_DIR for saving preprocessed datasets
TRAINED_DATA_DIR = BASE_DIR / "data" / "trained_data"
TRAINED_DATA_DIR.mkdir(parents=True, exist_ok=True)

TARGET_COL = "aqi_next1h"
EXPERIMENT_NAME = "aqi_forecasting"

RANDOM_STATE = 42
TEST_SIZE = 0.2


# ================= DATA LOADING =================

def find_latest_dataset_path() -> Path:
    """
    ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå training dataset ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å DATA_DIR
    pattern: aqi_lagged_SEA_YYYYMMDD_HHMMSS.csv

    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ timestamp ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ aqi_lagged_SEA.csv ‡πÄ‡∏õ‡πá‡∏ô fallback
    """
    pattern = "aqi_lagged_SEA_*.csv"
    files = sorted(DATA_DIR.glob(pattern))

    if files:
        latest = files[-1] 
        print(f"üîç ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå training dataset {len(files)} ‡πÑ‡∏ü‡∏•‡πå, ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {latest.name}")
        return latest

    # fallback: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏ö‡∏ö fix
    legacy = DATA_DIR / "aqi_lagged_SEA.csv"
    if legacy.exists():
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ timestamp ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå legacy ‡πÅ‡∏ó‡∏ô: {legacy.name}")
        return legacy

    raise FileNotFoundError(
        f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå training dataset ‡πÉ‡∏ô {DATA_DIR} "
        f"(‡∏ó‡∏±‡πâ‡∏á pattern aqi_lagged_SEA_*.csv ‡πÅ‡∏•‡∏∞ aqi_lagged_SEA.csv)"
    )


def load_dataset(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå data ‡∏ó‡∏µ‡πà {path}")
    df = pd.read_csv(path)
    print(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {path}, shape = {df.shape}")
    return df


def preprocess(df: pd.DataFrame):
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    df = df.drop_duplicates()
    print(f"‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡πâ‡∏ß shape = {df.shape}")
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà target ‡∏´‡∏≤‡∏¢
    if TARGET_COL not in df.columns:
        raise ValueError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå target '{TARGET_COL}' ‡πÉ‡∏ô dataset")

    df = df.dropna(subset=[TARGET_COL])

    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà feature ‡∏°‡∏µ NaN (‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    df = df.dropna()

    print(f"‡∏´‡∏•‡∏±‡∏á dropna ‡πÅ‡∏•‡πâ‡∏ß shape = {df.shape}")

    # ‡πÅ‡∏¢‡∏Å X, y
    y = df[TARGET_COL]

    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô feature
    cols_to_drop = {
        TARGET_COL,
        "station_idx",
        "station_name",
        "station_time",
    }
    cols_to_use = [c for c in df.columns if c not in cols_to_drop]

    X = df[cols_to_use]

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß
    X = X.select_dtypes(include=[np.number])

    print(f"‡πÉ‡∏ä‡πâ features ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {X.shape[1]} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {list(X.columns)}")

    return X, y


# ================= TRAINING =================

def train_and_log_model(model_name: str, model, X_train, X_test, y_train, y_test):
    """
    ‡πÄ‡∏ó‡∏£‡∏ô model ‡πÅ‡∏•‡πâ‡∏ß log ‡πÄ‡∏Ç‡πâ‡∏≤ MLflow 1 run
    """
    with mlflow.start_run(run_name=model_name):
        # log ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•
        mlflow.set_tag("model_name", model_name)

        # log parameters (‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ)
        if hasattr(model, "get_params"):
            mlflow.log_params(model.get_params())

        # train
        model.fit(X_train, y_train)

        # predict
        y_pred = model.predict(X_test)

        # metrics
        mae = mean_absolute_error(y_test, y_pred)
        rmse = mean_squared_error(y_test, y_pred) ** 0.5
        r2 = r2_score(y_test, y_pred)

        mlflow.log_metric("MAE", mae)
        mlflow.log_metric("RMSE", rmse)
        mlflow.log_metric("R2", r2)

        print(f"[{model_name}] MAE={mae:.4f}  RMSE={rmse:.4f}  R2={r2:.4f}")

        # log model (preferred) - This logs the model to DagsHub Artifact Store
        try:
            mlflow.sklearn.log_model(model, artifact_path="model")
            print("‚û°Ô∏è log model ‡πÄ‡∏Ç‡πâ‡∏≤ MLflow ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ (Artifacts ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å 'Push' ‡πÑ‡∏õ DagsHub)")
        except Exception as e:
            print("‚ö†Ô∏è ¬†‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ log model ‡πÄ‡∏Ç‡πâ‡∏≤ MLflow ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠):", e)
            
            # fallback: save model locally and upload it as a run artifact
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            local_model_path = MODELS_DIR / f"{model_name}_{timestamp}.pkl"
            
            try:
                joblib.dump(model, local_model_path)
                print(f"‚û°Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå local: {local_model_path}")
                
                # try to upload the saved file as a run artifact
                try:
                    # Logs the local file path into the run artifact, DagsHub handles the upload
                    mlflow.log_artifact(str(local_model_path), artifact_path="model_fallback")
                    print("‚û°Ô∏è ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô artifact ‡πÉ‡∏ô run ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                except Exception as e3:
                    print("‚ö†Ô∏è ¬†‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î artifact ‡πÑ‡∏õ‡∏¢‡∏±‡∏á MLflow ‡πÑ‡∏î‡πâ:", e3)
            except Exception as e2:
                print("‚ö†Ô∏è ¬†‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡∏á local ‡πÑ‡∏î‡πâ:", e2)

        # ‡∏Ñ‡∏∑‡∏ô metrics ‡πÑ‡∏ß‡πâ‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å best model ‡∏ï‡πà‡∏≠
        run_id = mlflow.active_run().info.run_id
        return {
            "model_name": model_name,
            "run_id": run_id,
            "mae": mae,
            "rmse": rmse,
            "r2": r2,
        }


def main():
    # ---------- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° MLflow experiment ----------
    mlflow.set_experiment(EXPERIMENT_NAME)
    print(f"‡πÉ‡∏ä‡πâ MLflow experiment: {EXPERIMENT_NAME}")

    # ---------- ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î & ‡πÇ‡∏´‡∏•‡∏î & preprocess data ----------
    try:
        data_path = find_latest_dataset_path()
    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        return 

    df = load_dataset(data_path)
    X, y = preprocess(df)

    # Save the preprocessed dataset
    preprocessed_data_path = TRAINED_DATA_DIR / f"preprocessed_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(preprocessed_data_path, index=False)
    print(f"‚û°Ô∏è Preprocessed dataset saved to: {preprocessed_data_path}")

    if len(X) < 50:
        print("‚ö† ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sample ‡∏ô‡πâ‡∏≠‡∏¢ (< 50) ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• overfit ‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏≠‡∏á pipeline ‡∏Å‡πà‡∏≠‡∏ô")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    print(f"Train size = {X_train.shape[0]} rows, Test size = {X_test.shape[0]} rows")

    # ---------- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á 3 ----------
    models = [
        ("LinearRegression", LinearRegression()),
        ("RandomForestRegressor", RandomForestRegressor(
            n_estimators=200,
            max_depth=None,
            random_state=RANDOM_STATE,
            n_jobs=-1,
        )),
        ("GradientBoostingRegressor", GradientBoostingRegressor(
            random_state=RANDOM_STATE
        )),
    ]

    results = []
    for name, model in models:
        print(f"\n===== Train {name} =====")
        res = train_and_log_model(name, model, X_train, X_test, y_train, y_test)
        results.append(res)

    # ---------- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏´‡∏≤ best model ----------
    print("\n===== Summary (sorted by RMSE) =====")
    results_sorted = sorted(results, key=lambda r: r["rmse"])
    for r in results_sorted:
        print(
            f"{r['model_name']:25s} | RMSE={r['rmse']:.4f} | MAE={r['mae']:.4f} | R2={r['r2']:.4f} | run_id={r['run_id']}"
        )

    best = results_sorted[0]
    print("\nBest model (RMSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î):")
    print(
        f"{best['model_name']} (run_id={best['run_id']}) "
        f"RMSE={best['rmse']:.4f}, MAE={best['mae']:.4f}, R2={best['r2']:.4f}"
    )

    # ---------- Auto-register best model ‡πÄ‡∏Ç‡πâ‡∏≤ Model Registry ----------
    model_name = "aqi_best_model"
    # Ensure the model is logged as 'model' artifact path for this URI to work, 
    # or use 'model_fallback' if only the fallback succeeded.
    model_uri = f"runs:/{best['run_id']}/model" 

    print(f"\nRegister best model ‡πÄ‡∏Ç‡πâ‡∏≤ Model Registry ‡∏ä‡∏∑‡πà‡∏≠ '{model_name}' ...")
    try:
        registered = mlflow.register_model(model_uri=model_uri, name=model_name)
        version = registered.version
        print(f" ¬† -> registered version = {version}")
        print("\n‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: best model ‡∏ñ‡∏π‡∏Å register ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        print(f" ¬† Model URI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ deploy: models:/{model_name}/{version}")
        print(" ¬† ‡∏î‡∏π‡∏ú‡∏•‡πÉ‡∏ô DagsHub: ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ Experiments ‡πÅ‡∏•‡∏∞ Model Registry")
    except Exception as e:
        print(f"‚ùå Error registering model: {e}")
        print(" ¬† ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Tracking Server (DagsHub) ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå (Auth) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")

    # ---------- Create a local folder called 'best_models' instead of pushing to DagsHub ----------
    BEST_MODELS_DIR = BASE_DIR / "best_models"
    BEST_MODELS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"‚û°Ô∏è Created local folder for best models: {BEST_MODELS_DIR}")

    # Update the logic to save the best model with its name and timestamp
    # Define the path for the best model artifact using the model name and timestamp
    best_model_artifact_path = BEST_MODELS_DIR / f"{best['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
    joblib.dump(model, best_model_artifact_path)
    print(f"‚û°Ô∏è Best model artifact saved locally: {best_model_artifact_path}")


if __name__ == "__main__":
    main()