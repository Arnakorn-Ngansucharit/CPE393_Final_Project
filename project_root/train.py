import os
from pathlib import Path

import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from mlflow.tracking import MlflowClient

# ================= CONFIG =================

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data" / "processed"   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DATA_PATH ‡πÄ‡∏õ‡πá‡∏ô DATA_DIR

TARGET_COL = "aqi_next1h"
EXPERIMENT_NAME = "aqi_forecasting"

RANDOM_STATE = 42
TEST_SIZE = 0.2


# ================= DATA LOADING =================

def find_latest_dataset_path() -> Path:
    """
    ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå training dataset ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å DATA_DIR
    pattern: aqi_lagged_SEA_YYYYMMDD_HHMMSS.csv

    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ timestamp ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ aqi_lagged_SEA.csv ‡πÄ‡∏õ‡πá‡∏ô fallback
    """
    pattern = "aqi_lagged_SEA_*.csv"
    files = sorted(DATA_DIR.glob(pattern))

    if files:
        latest = files[-1]  # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ YYYYMMDD_HHMMSS ‡∏ó‡∏≥‡πÉ‡∏´‡πâ sort ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏û‡∏≠‡∏î‡∏µ
        print(f"üîç ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå training dataset {len(files)} ‡πÑ‡∏ü‡∏•‡πå, ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {latest.name}")
        return latest

    # fallback: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏ö‡∏ö fix
    legacy = DATA_DIR / "aqi_lagged_SEA.csv"
    if legacy.exists():
        print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏°‡∏µ timestamp ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå legacy ‡πÅ‡∏ó‡∏ô: {legacy.name}")
        return legacy

    raise FileNotFoundError(
        f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå training dataset ‡πÉ‡∏ô {DATA_DIR} "
        f"(‡∏ó‡∏±‡πâ‡∏á pattern aqi_lagged_SEA_*.csv ‡πÅ‡∏•‡∏∞ aqi_lagged_SEA.csv)"
    )


def load_dataset(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå data ‡∏ó‡∏µ‡πà {path}")
    df = pd.read_csv(path)
    print(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {path}, shape = {df.shape}")
    return df


def preprocess(df: pd.DataFrame):
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    df = df.drop_duplicates()
    print(f"‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡πâ‡∏ß shape = {df.shape}")
    
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà target ‡∏´‡∏≤‡∏¢
    if TARGET_COL not in df.columns:
        raise ValueError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå target '{TARGET_COL}' ‡πÉ‡∏ô dataset")

    df = df.dropna(subset=[TARGET_COL])

    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà feature ‡∏°‡∏µ NaN (‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    df = df.dropna()

    print(f"‡∏´‡∏•‡∏±‡∏á dropna ‡πÅ‡∏•‡πâ‡∏ß shape = {df.shape}")

    # ‡πÅ‡∏¢‡∏Å X, y
    y = df[TARGET_COL]

    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô feature
    cols_to_drop = {
        TARGET_COL,
        "station_idx",
        "station_name",
        "station_time",
    }
    cols_to_use = [c for c in df.columns if c not in cols_to_drop]

    X = df[cols_to_use]

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß
    X = X.select_dtypes(include=[np.number])

    print(f"‡πÉ‡∏ä‡πâ features ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {X.shape[1]} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {list(X.columns)}")

    return X, y


# ================= TRAINING =================

def train_and_log_model(model_name: str, model, X_train, X_test, y_train, y_test):
    """
    ‡πÄ‡∏ó‡∏£‡∏ô model ‡πÅ‡∏•‡πâ‡∏ß log ‡πÄ‡∏Ç‡πâ‡∏≤ MLflow 1 run
    """
    with mlflow.start_run(run_name=model_name):
        # log ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•
        mlflow.set_tag("model_name", model_name)

        # log parameters (‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ)
        if hasattr(model, "get_params"):
            mlflow.log_params(model.get_params())

        # train
        model.fit(X_train, y_train)

        # predict
        y_pred = model.predict(X_test)

        # metrics
        mae = mean_absolute_error(y_test, y_pred)
        rmse = mean_squared_error(y_test, y_pred) ** 0.5
        r2 = r2_score(y_test, y_pred)

        mlflow.log_metric("MAE", mae)
        mlflow.log_metric("RMSE", rmse)
        mlflow.log_metric("R2", r2)

        print(f"[{model_name}] MAE={mae:.4f}  RMSE={rmse:.4f}  R2={r2:.4f}")

        # log model
        mlflow.sklearn.log_model(model, artifact_path="model")

        # ‡∏Ñ‡∏∑‡∏ô metrics ‡πÑ‡∏ß‡πâ‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å best model ‡∏ï‡πà‡∏≠
        run_id = mlflow.active_run().info.run_id
        return {
            "model_name": model_name,
            "run_id": run_id,
            "mae": mae,
            "rmse": rmse,
            "r2": r2,
        }


def main():
    # ---------- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° MLflow experiment ----------
    mlflow.set_experiment(EXPERIMENT_NAME)
    print(f"‡πÉ‡∏ä‡πâ MLflow experiment: {EXPERIMENT_NAME}")

    # ---------- ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î & ‡πÇ‡∏´‡∏•‡∏î & preprocess data ----------
    data_path = find_latest_dataset_path()
    df = load_dataset(data_path)
    X, y = preprocess(df)

    if len(X) < 50:
        print("‚ö† ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sample ‡∏ô‡πâ‡∏≠‡∏¢ (< 50) ‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• overfit ‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏≠‡∏á pipeline ‡∏Å‡πà‡∏≠‡∏ô")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    print(f"Train size = {X_train.shape[0]} rows, Test size = {X_test.shape[0]} rows")

    # ---------- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á 3 ----------
    models = [
        ("LinearRegression", LinearRegression()),
        ("RandomForestRegressor", RandomForestRegressor(
            n_estimators=200,
            max_depth=None,
            random_state=RANDOM_STATE,
            n_jobs=-1,
        )),
        ("GradientBoostingRegressor", GradientBoostingRegressor(
            random_state=RANDOM_STATE
        )),
    ]

    results = []
    for name, model in models:
        print(f"\n===== Train {name} =====")
        res = train_and_log_model(name, model, X_train, X_test, y_train, y_test)
        results.append(res)

    # ---------- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏´‡∏≤ best model ----------
    print("\n===== Summary (sorted by RMSE) =====")
    results_sorted = sorted(results, key=lambda r: r["rmse"])
    for r in results_sorted:
        print(
            f"{r['model_name']:25s} | RMSE={r['rmse']:.4f} | MAE={r['mae']:.4f} | R2={r['r2']:.4f} | run_id={r['run_id']}"
        )

    best = results_sorted[0]
    print("\nBest model (RMSE ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î):")
    print(
        f"{best['model_name']} (run_id={best['run_id']}) "
        f"RMSE={best['rmse']:.4f}, MAE={best['mae']:.4f}, R2={best['r2']:.4f}"
    )

    # ---------- Auto-register best model ‡πÄ‡∏Ç‡πâ‡∏≤ Model Registry ----------
    model_name = "aqi_best_model"
    model_uri = f"runs:/{best['run_id']}/model"

    print(f"\nRegister best model ‡πÄ‡∏Ç‡πâ‡∏≤ Model Registry ‡∏ä‡∏∑‡πà‡∏≠ '{model_name}' ...")
    registered = mlflow.register_model(model_uri=model_uri, name=model_name)
    version = registered.version
    print(f"   -> registered version = {version}")

    print("\n‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: best model ‡∏ñ‡∏π‡∏Å register ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    print(f"   Model URI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ deploy: models:/{model_name}/{version}")
    print("   ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π‡πÉ‡∏ô UI ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô: mlflow ui")


if __name__ == "__main__":
    main()